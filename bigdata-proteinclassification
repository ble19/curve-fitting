#update Hdf5 to 1.10.3
import zipfile

from pyspark import SparkConf, SparkContext
import imageio
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import mahotas.features
from elephas.utils.rdd_utils import to_simple_rdd
from elephas.spark_model import SparkModel
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.optimizers import SGD
# %matplotlib inline

def fill_targets(row):
    row.Target = np.array(row.Target.split(" ")).astype(np.int)
    for num in row.Target:
        name = label_names[int(num)]
        row.loc[name] = 1
    return row
# takes the labels from the base kaggle_dataser and put it in a form for sklearn one-vs-rest and other multilabel classifiers
def sparse_label(column):
    sparseLabels = []
    column = pd.DataFrame(column)
    for row in column.iterrows():
        for i in row[1].values:
            label = i.split()
        label = [int(i) for i in label]
        sparse = pd.DataFrame(np.zeros((1, 28), dtype = int))
        for il in label:
            for j in sparse:
                if il == j:
                    sparse[j] = 1

        sparseLabels.append(sparse)
    return np.vstack(sparseLabels)
sns.set()

'''import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)'''


# allows elephas to use spark to run a keras NN
conf = SparkConf().setAppName('Elephas_App').setMaster('local[8]')
sc = SparkContext(conf=conf)

'''Below is initial code kindly provided by the Kaggle user allunia providing 
a kernal for data exploration of the dataset'''
#filepath = 'C:\\Users\\blela\\PycharmProjects\\kaggle_protein_identification\\train.csv'
train_labels = pd.read_csv('section_kaggle.csv', sep = ',')

label_names = {
    0:  "Nucleoplasm",
    1:  "Nuclear membrane",
    2:  "Nucleoli",
    3:  "Nucleoli fibrillar center",
    4:  "Nuclear speckles",
    5:  "Nuclear bodies",
    6:  "Endoplasmic reticulum",
    7:  "Golgi apparatus",
    8:  "Peroxisomes",
    9:  "Endosomes",
    10:  "Lysosomes",
    11:  "Intermediate filaments",
    12:  "Actin filaments",
    13:  "Focal adhesion sites",
    14:  "Microtubules",
    15:  "Microtubule ends",
    16:  "Cytokinetic bridge",
    17:  "Mitotic spindle",
    18:  "Microtubule organizing center",
    19:  "Centrosome",
    20:  "Lipid droplets",
    21:  "Plasma membrane",
    22:  "Cell junctions",
    23:  "Mitochondria",
    24:  "Aggresome",
    25:  "Cytosol",
    26:  "Cytoplasmic bodies",
    27:  "Rods & rings"
}
for key in label_names.keys():
    train_labels[label_names[key]] = 0

train_labels = train_labels.apply(fill_targets, axis=1)
train_labels.head()

reverse_train_labels = dict((v,k) for k,v in label_names.items())


target_counts = train_labels.drop(["Id", "Target"],axis=1).sum(axis=0).sort_values(ascending=False)
plt.figure(figsize=(15,15))
sns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index)
#plt.show()
# image_unprocessed =
image_keys = pd.read_csv('section_kaggle.csv')
zip_data = zipfile.ZipFile('train.zip', 'r')
labels = sparse_label(image_keys['Target'])
image_set = []
for i, row in zip(image_keys['Id'], labels):
    image_set.append((i, row))
image_set = pd.DataFrame(image_set)
image_matrices = []
for i in image_set.iterrows():
    regex_green = i[1][0] + r"_green.png"
    imggreen = zip_data.read(regex_green)
    img_green = imageio.imread(imggreen)
    image_green = np.reshape(img_green, (512, 512))
    #print(np.size(imggreen))
    #green = np.frombuffer(imggreen, dtype= np.uint8).reshape(512, 512)
    image_matrices.append([np.vstack(image_green), i[1][1]])
    # add rest of dataset building
images = np.array(image_matrices)
